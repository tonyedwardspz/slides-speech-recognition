<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Empowering Web Applications with Speech Recognition - Mihai Cîrlănaru</title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/night.css">
    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/atom-one-dark.css">
    <!-- Custom styles -->
    <link rel="stylesheet" href="css/styles.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">

<!-- SLIDE 1 -->
<section data-background-image="img/full-ink.jpg" data-transition="slide" data-state="intro">
  <h2>Empowering Web Applications with</h2>
  <h3 class="title-main">
    <span class="voice-visualizer">
      <svg preserveAspectRatio="none" id="visualizer" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
        <defs>
          <mask id="mask">
            <g id="maskGroup"></g>
          </mask>
          <linearGradient id="gradient" x1="0%" y1="0%" x2="0%" y2="100%">
            <stop offset="0%" style="stop-color:rgba(250, 250, 250, 1.0);stop-opacity:1" />
            <stop offset="20%" style="stop-color:rgba(215, 217, 221, 1.0);stop-opacity:1" />
            <stop offset="90%" style="stop-color:rgba(100, 101, 102, 1.0);stop-opacity:1" />
            <stop offset="100%" style="stop-color:rgba(56, 67, 79, 1.0);stop-opacity:1" />
          </linearGradient>
        </defs>
        <rect x="0" y="0" width="100%" height="100%" fill="url(#gradient)" mask="url(#mask)"></rect>
      </svg>
    </span>
    <span class="emphasize title-main__em">Speech Recognition</span>
  </h3>
  <div class="name">
    <div class="name-text">Mihai Cîrlănaru</div>
    <div class="name-sub">@MCirlanaru</div>
  </div>
  <div class="main">
    <div class="main-container">
      <i class="conf-logo"></i>
    </div>
  </div>

  <aside class="notes">
    Thanks for having me here, HalfStack is one of those hidden gems of web development conferences,
    I'm very excited about being in front of you today.
    <br><br>
    For the next half an hour I will be talking to you about a topic I am quite passionate about, Speech Recognition.
    <b>WHY</b> it's a such a big deal these days, <b>HOW</b> it works, and <b>HOW</b> to use it on the web
  </aside>
</section>

<!-- SLIDE 2 -->
<section data-background-image="img/hudl-bg.jpg" data-transition="zoom-in fade-out">
  <div class="name-text">Mihai Cîrlănaru</div>
  <div class="name-sub">Software Development Lead</div>
  <i class="hudl-logo"></i>
  <aside class="notes">
    First off, a bit about myself, my name is Mihai, almost did a PhD in NLP a few years back, but went to the
    dark side (joined a startup) and now I'm a team lead based in Germany, working on full-stack web for Hudl,
    a sports technology company that builds performance analysis tools for coaches and athletes.
  </aside>
</section>

<!-- <section data-background-image="img/hudl-video.mp4" data-background-video-loop=true>
  <aside class="notes">
    More than 143,000 total active teams across all sports and all levels
    Totals 5 million active users - coaches, athletes, trainers and analysts
    99% of American Football teams use Hudl, all 20 premier league teams use our software and 29/30 NBA teams...
  </aside>
</section> -->

 <!-- SLIDE 3 Intro -->
 <section data-background-image="img/title-slide-1.png" data-transition="slide">
   <p>&nbsp;</p>
   <aside class="notes">
     Speech Recognigion has been around for decades (early 1950s) but because it tackles such a HARD problem (just think of
     all the factors that play into it: accent, pronunciation, pitch, volume, and speed) it was mainly targetting
     highly specialized domains:<br>
     &nbsp;&nbsp;- speaker dependent: involves a training step where a user is asked to read some sentences -> fine tune<br>
     &nbsp;&nbsp;- command based/limited vocab: e.g. call routing, digit recognition<br>
     But then, technology caught up and starting with Apple's Siri in 2011 a wave a voice-enabled devices/personal
     assistants started to pick up.
   </aside>
 </section>

<!-- SLIDE 3 -->
<section data-background-image="img/siri.jpg" data-transition="fade-out">
  <p>&nbsp;</p>
  <aside class="notes">
    <i>But then, technology caught up and starting with Apple's Siri in 2011 a wave a voice-enabled devices/personal
    assistants started to pick up.</i><br>
    While not perfect back then, Siri opened the door to a wide range of voice-enabled interactions, making
    technology more intuitive and even fun.<br><br>
    How many of you used Siri so far?
  </aside>
</section>

<!-- SLIDE 4 -->
<section data-background-image="img/alexa-2.jpg" data-transition="fade-out">
  <p>&nbsp;</p>
  <aside class="notes">
    A few years later (2014), Amazon took the voice-enabled assistant to a whole new level, these black cylindrical
    speakers with the glowing blue LED ring quickly became the staple of home automation and pretty much
    kicked off a whole new market of IoT devices that can be controlled with one's voice.<br><br>
    How many of you used Alexa so far?
  </aside>
</section>

<!-- SLIDE 5 -->
<section data-background-image="img/google-home.webp" data-transition="fade-out">
  <p>&nbsp;</p>
  <aside class="notes">
    While Google did have voice input for search, it was only in 2016 that they created their own clever home decoration
    appliance in the shape of a flower-vase.<br><br>
    How many of you used Google voice input?<br><br>
    And as it turns out, more and more of these voice-enabled devices are coming up soon.
  </aside>
</section>

<!-- SLIDE 6 -->
<section data-background-image="img/voice-speakers.png" data-transition="fade-out">
  <p>&nbsp;</p>
  <aside class="notes">
    <i>And as it turns out, more and more of these voice-enabled devices are coming up soon</i><br>
    Drawing a parallel to the smartphone revolution, this might just be the Voice Speaker Revolution.<br><br>
    And as we saw here on a smaller scale, there are actually a lot of people using these -- in the US alone
  </aside>
</section>

<!-- SLIDE 7 -->
<section data-background-image="img/voice-users.png" data-transition="fade-out">
  <p>&nbsp;</p>
  <aside class="notes">
    <i>In the US alone</i> approx 55 million people use a voice enabled digital assistant,
    with the trend going upward.<br><br>
    But how come this exploded now?
  </aside>
</section>

<!-- SLIDE 8 -->
<section data-background-image="img/voice-performance.png" data-transition="fade-out">
  <p>&nbsp;</p>
  <aside class="notes">
    Well, speech recognition got very GOOD these days -- in terms of numbers (and I'll explain these in a bit)
    Google managed to improve their own speech recognizer's error rate from 23% to 4.9% in a handful of years and
    Baidu has currently knocked that out of the park with a 3.7% error rate achieved by its Deep Speech 2 system.
  </aside>
</section>

<!-- SLIDE 9 -->
<section data-background-image="img/performance.png" data-transition="fade-out">
  <p>&nbsp;</p>
  <aside class="notes">
    If you are wondering what this error rate actually means: it is the sum of all errors the system produces over
    the size of the reference speech. These errors can be Insertions, Substitutions and Deletions.
    <br><br>
    Then sure, you might say nice numbers and math, but what does that mean?
    It turns out speech recognition systems are so good that they can even beat humans to it.
    And there's research to prove it -- a group from Stanford proved that
  </aside>
</section>

<!-- SLIDE 10 -->
<section data-background-image="img/voice-research.png" data-transition="fade-out">
  <p>&nbsp;</p>
  <aside class="notes">
    A group from Stanford used the speech rec tech from Baidu (the 3.7% one from earlier) and tested it
    vs people at transcribing speech -- and guess what it is better, 3x faster -- and if you check out the
    research at that link you will also learn that it performs better in terms of correctness for mandarin.<br><br>
    And if you used one of these devices you will not be surprised at all by these results --
    I personally have no idea how the app to set reminders or timers looks like, I just use Alexa or Siri.
    <br><br>
    Now that we've seen how big of deal it is let's see how it works
  </aside>
</section>

<!-- SLIDE 11 Intro -->
<section data-background-image="img/title-slide-2.png" data-transition="slide">
  <p>&nbsp;</p>
  <aside class="notes">
    Now that we've seen how big of deal it is let's see how it works
  </aside>
</section>

<!-- SLIDE 11 -->
<section data-background-image="img/speechwave-1.png" data-transition="fade">
  <p>&nbsp;</p>
  <aside class="notes">
    What our voice produces is a continuous sound wave whose amplitude varies based on the sounds involved.<br><br>
    The recognition process works as follows
  </aside>
</section>

<!-- SLIDE 12 -->
<section data-background-image="img/speechwave-2.png" data-transition="fade">
  <p>&nbsp;</p>
  <aside class="notes">
    Split the waveform in utterances (words or other non-linguistic sounds) by silences
  </aside>
</section>

<!-- SLIDE 13 -->
<section data-background-image="img/speechwave-3.png" data-transition="fade">
  <p>&nbsp;</p>
  <aside class="notes">
    Each utterance is split in frames and for each frame a feature vector is computed -- a set of numbers representing
    distinguishable properties of a sound
  </aside>
</section>

<!-- SLIDE 14 -->
<section data-background-image="img/speechwave-4.png" data-transition="fade">
  <p>&nbsp;</p>
  <aside class="notes">
    Using a series of models, the search space for a match is significantly reduced and thus recognition is made
    possible in a finite amount of time:<br>
    - <b>acoustic model</b> contains acoustic properties for each distinct unit of sound (senone)
    - <b>phonetic dictionary</b> provides information on how to pronounce each word
    - <b>language model</b> is used to restrict word search
  </aside>
</section>

<!-- SLIDE 14' -->
<section data-background-image="img/speechwave-5.png">
  <p>&nbsp;</p>
  <aside class="notes">
    And for the speech represented here the decoding would be "Hello World" -- just in case you wondered
    how speech recognizers are assessed<br><br>
    Now that we got this out of the way, let's get to the fun part
  </aside>
</section>

<!-- SLIDE 15 Intro -->
<section data-background-image="img/title-slide-3.png" data-transition="slide">
  <p>&nbsp;</p>
  <aside class="notes">
    <i>Now that we got this out of the way, let's get to the fun part.</i>
    And as every good talk should have a good story I'm going to tell you one about my experience with
    Speech Recognition on the Web.
  </aside>
</section>

<!-- SLIDE 15 -->
<section data-background-image="img/longtimeago.png" data-transition="fade">
  <p>&nbsp;</p>
  <aside class="notes">
    I started playing around with speech recognition a few years back when I was studying about it for
    my Masters degree and got this great opportunity to put in practice what I learned about speech recognition
    within the Research Team at this small little tech company in the Valley
  </aside>
</section>


<!-- SLIDE 16 -->
<section data-background-image="img/mozilla.jpg" data-transition="fade">
  <i class="mozilla-logo"></i>
  <aside class="notes">
    Within the Research Team at Mozilla, to build a speech recognition engine for the now defunct Firefox OS
  </aside>
</section>

<!-- SLIDE 16' -->
<section data-background-image="img/firefox-os3.jpg" data-transition="fade">
  <aside class="notes">
    Mozilla’s mobile phone operating system built entirely using web technologies. RIP it was a great<br><br>
    Being a research project, the focus was more on exploring what is even possible in terms of speech recognition
    for the web, rather than have something production ready.
  </aside>
</section>

<!-- SLIDE 17 -->
<section data-background-image="img/full-ink.jpg">
  <div class="name-text">DEMO</div>
  <div class="name-sub emphasize">github.com/mihai/talk.js</div>
  <i class="github-repo hero-pic">&nbsp;</i>

  <aside class="notes">
    During my time there, I have built a pure JavaScript speech recognizer that performs the decoding process
    on the client, without the need of any cloud-based backend to send the speech to for decoding.<br>
    And I'm going to show it to you -- to make it even more exciting I'm going to kill the network on this --
    don't try this in live demos
    <br><br>
    If you are interested more on how this was built, and how it works behind the scenes,
    feel free to grab me after the talk.
  </aside>
</section>

<!-- SLIDE 18 -->
<section data-background-image="img/full-ink.jpg">
  <div class="name-text">Web Speech API Spec</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechSpec</div>
  <i class="spec hero-pic">&nbsp;</i>
  <aside class="notes">
    Little did I know, around the same time, a team from Google were writing a spec for Web Speech APIs
    -- defining JavaScript APIs that would allow developers to incorporate speech recognition into their web pages.
    <br>About a year later, with Chrome v25, Web Speech APIs were out. Whereas Chrome had speech input capabilities
    with x-webkit-speech attribute (now deprecated) to input fields since v11.
    <br><br>
    Ok, so let’s see how these APIs look like in practice
  </aside>
</section>

<!-- SLIDE 19 -->
<section data-background-image="img/full-ink.jpg">
  <div class="name-text">Web Speech API</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechAPI</div>

<pre>
<code data-trim data-noescape>
const SpeechRecognition = SpeechRecognition || webkitSpeechRecognition;
let recognizer = null;

if (!SpeechRecognition) {
  console.error("Web Speech APIs not supported in your browser");
} else {
  recognizer = new SpeechRecognition();
}
</code>
</pre>
  <aside class="notes">
    You first need to instantiate a Speech Recognition object, and since the APIs are still experimental they
    do need the browser prefix for Chrome (not for Firefox though).
  </aside>
</section>

<!-- SLIDE 20 -->
<section data-background-image="img/full-ink.jpg">
  <div class="name-text">Speech Recognition Attributes</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechAPI</div>
<pre>
<code data-trim data-noescape>
// Sets the language to be recognized (32 languages
// supported, incl. Romanian)
recognizer.lang = 'en-US';

// Get recognition results as early as possible,
// even if they will change
recognizer.interimResults = true;

// Continuously listen to speech, regardless if
// the user takes pauses or not
recognizer.continuous = true;

// The number of alternative recognition
// matches to be returned
recognizer.maxAlternatives = 3;

…
</code>
</pre>
  <aside class="notes">
    Some of the most relevant configuration attributes of the speech recognition instance.
  </aside>
</section>

<!-- SLIDE 21 -->
<section data-background-image="img/full-ink.jpg">
  <div class="name-text">Speech Recognition Event Handlers</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechAPI</div>
<pre>
<code data-trim data-noescape>
recognizer.onresult // Whenever a speech recognition match is found
recognizer.onnomatch // When no match was found for the current speech
recognizer.onerror // When an error occurred
…
</code>
</pre>
  <aside class="notes">
    There are a few event handlers provided for the Speech Recognition interface, and I would recommend you check
    the full API at http://bit.ly/WebSpeechAPI, however the minimum needed to get you started are:<br>
    onresult, onnomatch, onerror
  </aside>
</section>

<!-- SLIDE 22 -->
<section data-background-image="img/full-ink.jpg">
  <div class="name-text">Speech Recognition Control</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechAPI</div>
<pre>
<code data-trim data-noescape class="js">
recognizer.start();
recognizer.abort();
recognizer.stop();
</code>
</pre>
  <aside class="notes">
    In terms of controlling the recognizer we have a pretty self-explanatory API: start and stop.<br>
    And one last piece to the puzzle, this is how a Speech Recognition onResult event format looks like.
  </aside>
</section>

<!-- SLIDE 23 -->
<section data-background-image="img/full-ink.jpg">
  <div class="name-text">Speech Recognition Result format</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechAPI</div>
<pre>
<code data-trim data-noescape class="js">
// results[i][0].transcript -- top confidence transcription for result i
{
  results: [ // SpeechRecognitionResultList
    [ // SpeechRecognitionResult
      { // SpeechRecognitionAlternative
        transcript: "hello",
        confidence: 0.8999,
        isFinal: false
      },
      { // SpeechRecognitionAlternative
        transcript: "world",
        confidence: 0.4792,
        isFinal: false
      }
    ],
  ]
  …
}
</code>
</pre>
  <aside class="notes">
    SpeechRecognitionResultList -- each result has multiple alternatives results[0][0].transcript to get the
    first result with highest probability<br>
    isFinal -- this result is not an interim result and thus it will not change
  </aside>
</section>

<!-- SLIDE 24 -->
<section data-background-image="img/full-ink.jpg">
  <div class="name-text">Privacy</div>
  <div class="name-sub emphasize">bit.ly/WebSpeechAPI</div>
  <i class="privacy hero-pic">&nbsp;</i>
  <aside class="notes">
    In terms of privacy, Chrome needs to ask the user for permission to use the microphone, in which case onstart only
    fires when and if the user allows permission.<br>
    Pages hosted on HTTPS do not need to ask repeatedly for permission, whereas HTTP hosted pages do.<br>
    And this brings us to the last point about the WebSpeech API
  </aside>
</section>

<!-- SLIDE 25 -->
<section data-background-image="img/full-ink.jpg" data-transition="slide-in fade-out">
  <div class="name-text">Browser Support</div>
  <div class="name-sub emphasize">caniuse.com/#feat=speech-recognition</div>
  <i class="browser-support hero-pic">&nbsp;</i>
  <aside class="notes">
    As you can see support is not that great, it’s only Chrome and Opera that support it out of the box,
    whereas Firefox needs the webspeech recognition flag set. Microsoft is currently working on it for Edge.
    <br><br>
    This image might look grim, but hey the global browser usage percentage is 60% and FF supports it behind a config flag
    and this shouldn't discourage you from experimenting with new ways of improving UX using voice in your web apps.
    And on that note,
  </aside>
</section>

<!-- SLIDE 26 -->
<section data-background-image="img/hudl-layout-main.png" data-transition="fade" data-state="voice-visualizer speech-start">
  <h3><span class="voice-visualizer">
    <svg preserveAspectRatio="none" id="visualizer-end" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
      <defs>
        <mask id="mask-end">
          <g id="maskGroup"></g>
        </mask>
        <linearGradient id="gradient-end" x1="0%" y1="0%" x2="0%" y2="100%">
          <stop offset="0%" style="stop-color:rgba(250, 250, 250, 1.0);stop-opacity:1" />
          <stop offset="20%" style="stop-color:rgba(215, 217, 221, 1.0);stop-opacity:1" />
          <stop offset="90%" style="stop-color:rgba(255, 140, 0, 1.0);stop-opacity:1" />
          <stop offset="100%" style="stop-color:rgba(255, 99, 0, 1.0);stop-opacity:1" />
        </linearGradient>
      </defs>
      <rect x="0" y="0" width="100%" height="100%" fill="url(#gradient-end)" mask="url(#mask-end)"></rect>
    </svg>
  </span></h3>
  <div id="results">
    <span id="final_span" class="final"></span>
    <span id="interim_span" class="interim"></span>
  </div>
  <div class="name">
    <div class="name-text">Mihai Cîrlănaru</div>
    <div class="name-sub">@MCirlanaru</div>
  </div>
  <div class="main">
    <div class="main-container">
      <i class="conf-logo-secondary"></i>
    </div>
  </div>
  <aside class="notes">
    Just imagine, how cool would it be to have a live transcription of your talk using speech recognition, on the web
    how crazy is that? Thank you
  </aside>
</section>

      </div>
    </div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>

var paths = document.getElementsByTagName('path');
var visualizer = document.getElementById('visualizer');
var visualizerEnd = document.getElementById('visualizer-end');
var mask1 = visualizer.getElementById('mask');
var mask2 = visualizerEnd.getElementById('mask-end');
var path;
var NR_LINES = 40;

var soundAllowedCallback = function (stream) {
    //Audio stops listening in FF without // window.persistAudioStream = stream;
    //https://bugzilla.mozilla.org/show_bug.cgi?id=965483
    //https://support.mozilla.org/en-US/questions/984179
    window.persistAudioStream = stream;
    var audioContent = new AudioContext();
    var audioStream = audioContent.createMediaStreamSource( stream );
    var analyser = audioContent.createAnalyser();
    audioStream.connect(analyser);
    analyser.fftSize = 1024;

    var frequencyArray = new Uint8Array(analyser.frequencyBinCount);
    visualizer.setAttribute('viewBox', `0 0 ${NR_LINES} 255`);
    visualizerEnd.setAttribute('viewBox', `0 0 ${NR_LINES} 255`);

    //Through the frequencyArray has a length longer than 255, there seems to be no
    //significant data after this point. Not worth visualizing.
    for (var i = 0 ; i < NR_LINES; i++) {
        path1 = document.createElementNS('http://www.w3.org/2000/svg', 'path');
        path2 = document.createElementNS('http://www.w3.org/2000/svg', 'path');
        mask1.appendChild(path1);
        mask2.appendChild(path2);
    }
    var doDraw = function () {
        requestAnimationFrame(doDraw);
        analyser.getByteFrequencyData(frequencyArray);
        var adjustedLength;
        for (var i = 0 ; i < NR_LINES; i++) {
            adjustedLength = Math.floor(frequencyArray[i]) - (Math.floor(frequencyArray[i]) % 5);
            paths[i].setAttribute('d', 'M '+ (i) +',255 l 0,-' + adjustedLength);
        }

    }
    doDraw();
}

navigator.getUserMedia({ audio: true }, soundAllowedCallback, function(){});

var two_line = /\n\n/g;
var one_line = /\n/g;
var thank_you = /\s+thank\s+you/gi;
function linebreak(s) {
  return s.replace(two_line, '<p></p>').replace(one_line, '<br>');
}
var first_char = /\S/;
function capitalize(s) {
  return s.replace(first_char, function(m) {
    return m.toUpperCase();
  }).replace(thank_you, '<p class="thanks">thank you</p>');
}

var final_transcript = '';
var final_span = document.getElementById('final_span');
var interim_span = document.getElementById('interim_span');
var listeningToSpeech = false;
var recognizerTimeout = null;
var recognizer = {};

var isSpeakerNotes = document.URL.indexOf('?receiver') !== -1;

function resetSpeech() {
  final_transcript = '';
  listeningToSpeech = false;
  final_span.innerHTML = '';
  interim_span.innerHTML = '';

  if (!isSpeakerNotes) {
    // Setup speech recognition
    if (recognizer.abort) recognizer.abort();

    recognizer = new webkitSpeechRecognition();
    recognizer.continuous = true;
    recognizer.lang = "en-US";
    recognizer.maxAlternatives = 3;
    recognizer.interimResults = true;
  }
}

// Initialize speech recognizer
resetSpeech();

// More info https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
  history: true,
  controls: false,
  viewDistance: 5,
  keyboard: {
    // ENTER KEY
    13: function() {
      if (!listeningToSpeech) {
        console.info('KEY SPEECH START');
        recognizer.start();
        listeningToSpeech = true;
      } else {
        console.info('KEY SPEECH END');
        recognizer.stop();
        listeningToSpeech = false;
      }
    },
    // SPACE KEY
    32: function() {
      recognizer.abort();
      final_transcript = '';
      listeningToSpeech = false;
      final_span.innerHTML = '';
      interim_span.innerHTML = '';

      console.warn('RESET SPEECH RECOGNIZER');

      recognizerTimeout = setTimeout(function(){
        console.info('SPEECH STARTING...');
        recognizer.start();
        listeningToSpeech = true;
      }, 500);
    },
    // UP KEY
    38: 'prev',
    40: 'next'
  },
  // width: 1920,
  // height: 1080,
  width: 1280,
  height: 720,

  // More info https://github.com/hakimel/reveal.js#dependencies
  dependencies: [
    { src: 'plugin/markdown/marked.js' },
    { src: 'plugin/markdown/markdown.js' },
    { src: 'plugin/notes/notes.js', async: true },
    { src: 'plugin/zoom-js/zoom.js', async: true },
    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
  ]
});

if (!isSpeakerNotes) {
  recognizer.onend = function(event) {
    console.warn('SPEECH END');
    listeningToSpeech = false;
    final_transcript += ' ';
  }

  recognizer.onresult = function(event) {
    var interim_transcript = '';
    for (var i = event.resultIndex; i < event.results.length; ++i) {
      if (event.results[i].isFinal) {
        final_transcript += event.results[i][0].transcript;
      } else {
        interim_transcript += event.results[i][0].transcript;
      }
    }
    final_transcript = capitalize(final_transcript);
    final_span.innerHTML = linebreak(final_transcript);
    interim_span.innerHTML = linebreak(interim_transcript);

    console.group('SPEECH RESULT');
    console.info('INTERIM:', interim_transcript);
    console.info('FINAL:', final_transcript);
    console.groupEnd();
  };

  recognizer.onerror = function(event) {
    console.error('ERROR:', event.error);
    if (event.error === 'network' || event.error === 'aborted') {
      console.info('SPEECH RESTART');
      recognizer.stop();
      if (recognizerTimeout) {
        clearTimeout(recognizerTimeout);
        recognizerTimeout = null;
      }
      recognizerTimeout = setTimeout(function(){
        console.info('SPEECH STARTING...');
        recognizer.start();
        listeningToSpeech = true;
      }, 500);
    }
  };

  Reveal.addEventListener( 'slidechanged', function( event ) {
    // event.previousSlide, event.currentSlide, event.indexh, event.indexv
    console.info('SLIDE CHANGED', event.indexh);
  } );

  Reveal.addEventListener( 'speech-start', function(ev) {
    // Start recognition
    recognizer.start();
    listeningToSpeech = true;
    console.info('SPEECH START SLIDE')
  }, false );

  Reveal.addEventListener( 'intro', function(ev) {
    var visualizer = document.getElementById('visualizer');
    paths = document.getElementsByTagName('path');
  }, false );

  Reveal.addEventListener( 'voice-visualizer', function(ev) {
    var visualizer = document.getElementById('visualizer-end');
    paths = visualizer.getElementsByTagName('path');
  }, false );
}
</script>
  </body>
</html>
